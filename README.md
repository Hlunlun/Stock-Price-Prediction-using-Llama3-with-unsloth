# Stock Price Prediction using Llama3 with unsloth

## è³‡æ–™ä¾†æº

### **ç›¸é—œå…¬å¸é‡‘èé ˜åŸŸæ–°è**

å¯ä»¥ç”¨ RAG æ–¹å¼åœ¨ inference éšæ®µæ”¾åˆ° context çš„éƒ¨åˆ†æä¾› LLM é¡å¤–è³‡è¨Šï¼Œä»¥ä¸‹çš„ API éƒ½æ˜¯ä½¿ç”¨å…è²»ç‰ˆï¼Œæ‰€ä»¥æœ€å¤šåªèƒ½å–å¾— è‡³ä»Šé“ 30 å¤©å‰çš„æ–°èï¼Œè¨»å†Šå–å¾—å€‹äºº token å³å¯ä½¿ç”¨

- [`newsapi`](https://newsapi.org/) : æœ‰æ”¯æŒä¸­æ–‡æ–°èå’Œç¯©é¸å…¬å¸ï¼Œä½†ä¸€å¤©åªæœ‰ 100 ç­†è³‡æ–™
- [`yfinance`](https://github.com/ranaroussi/yfinance) : æ¯å¤©ä¹Ÿæœ‰é™åˆ¶å¯å–å¾—çš„æ¬¡æ•¸ï¼Œä¸”æ²’æœ‰ä¸­æ–‡

æ‰€ä»¥é¸æ“‡ä½¿ç”¨ [`newsapi`](https://newsapi.org/) ä¾†å–å¾—ç›¸é—œæ–°è

### **å”åŠ©åˆ¤åˆ¥å·¥å…·**

- [`twstock`](https://github.com/mlouielu/twstock) : æä¾›ç°¡å–®çš„è²·è³£æŒ‡æ¨™ï¼Œå¯ä»¥æä¾›æ¨¡å‹è¼”åŠ©è³‡è¨Šä¾†é æ¸¬è‚¡ç¥¨åƒ¹æ ¼

### **å…¬å¸é¸æ“‡**

åƒè€ƒ:   [ä¸­å¤®é€šè¨Šç¤¾ Central News Agency é»ƒä»å‹³æ¼”è¬›èƒŒæ¿åˆä½œç«æ¿](https://www.cna.com.tw/news/afe/202406030108.aspx)

é¸æ“‡æœ€èˆ‡æ™¶ç‰‡ç›¸é—œçš„å…¬å¸ï¼Œåƒè€ƒèˆ‡ NVIDIA åˆä½œå…¬å¸ï¼Œä»¥åŠä¸€äº›é‡‘èè‚¡ï¼Œä¸¦ä¸”æ•´ç†å‡ºæœ‰ä¸Šå¸‚ä¸”å¯é€é `twstock` å–å¾—è‚¡ç¥¨æ­·å²åƒ¹æ ¼ä»¥åŠç›¸é—œæ–°èçš„å…¬å¸å¦‚ä¸‹ 77 å®¶å…¬å¸

| ç ”æš | å®ç¢ | å‡Œè¯ | å®‰æåœ‹éš› | è‹±ç ”æ™ºèƒ½ | å·¨å¤§ | é›²é”ç§‘æŠ€ |
| --- | --- | --- | --- | --- | --- | --- |
| åŸæ™ºç§‘æŠ€ | å®‰å®ç”Ÿé†« | äºå¤ªæ™ºèƒ½æ©Ÿå™¨ | ä¹‹åˆåŠ é€Ÿå™¨ | æ—¥æœˆå…‰ | æŠ€å˜‰ | å»£é” |
| è¯æ“ | è¯ç¢© | è‰¾å¾®åµå¥‡ | åœ“å‰› | è‰¾è¨Š | é›†é›…ç§‘æŠ€ | æ‰€ç¾…é–€ |
| æ¯”åƒ¹ç‹ | å‹¤èª  | é›†ä»•å¤š | ä¸ƒå½©è™¹ | ä»å¯¶ | å¼˜å„„è‚¡ | çŸ½å“ |
| ç´¢æ³° | ä¸­å…‰é›» | æ»¿æ‹“ç§‘æŠ€ | ç¥ç‘äººå·¥æ™ºæ…§ | å°é”é›» | åˆå®œå®¶å±… | å„’æ· |
| é©è‘—ä¸‰ç¶­ç§‘æŠ€ | æ•¦æ–°ç§‘æŠ€ | ç›Šç™»ç§‘æŠ€ | æ…§å‹ | é´»æµ· | æ•¸ä½ç„¡é™è»Ÿé«” | é£›æ· |
| é´»ä½° | æ˜ çœ¾ | è‹±æ¥­é” | è¿å»£ | å»£é‹ | äº¬å…ƒé›» | æ¾„é¢¨ç§‘æŠ€ |
| ç«‹ç«¯ | éº—è‡º | æ¨‚é”ç§‘æŠ€ | å¾‹æœç§‘æŠ€ | å…‰å¯¶ç§‘ | é‘«è–©æ—ç§‘ | ç¾è¶…å¾® |
| è¬åˆ©é” | è¯ç™¼ç§‘ | å®‡è¦‹æ™ºèƒ½ç§‘æŠ€ | ç¥é” | å¾®æ˜Ÿ | åº«æ›œç§‘æŠ€ | é”æ˜æ©Ÿå™¨äºº |
| æ–°æ¼¢ | é†«æš | åŒå¾· | å’Œç¢© | å¾·å¾‹ | æ›œè¶Š | è¶¨å‹¢ç§‘æŠ€ |
| å°ç©é›» | è¯é›» | æ¬£èˆˆ | è¶…æ© | ç·¯å‰µ | ç·¯ç© | è°æ³° |

## è³‡æ–™å‰è™•ç†

1. ç§»é™¤æ²’æœ‰ä¸Šå¸‚çš„å…¬å¸
    
    æœ‰äº›æ²’è¾¦æ³•ç”¨ `twstock` çš„å–å¾—è‚¡ç¥¨ç·¨è™Ÿï¼Œæˆ–æ˜¯æ ¹æœ¬æ²’æœ‰ä¸Šå¸‚çš„å…¬å¸ä¹Ÿæ²’è¾¦æ³•é€²è¡Œè‚¡ç¥¨æŠ•è³‡ï¼Œæœ€å¾Œç¯©é¸å‡ºä»¥ä¸‹ 41 å…¬å¸
    
    | ç ”æš | å®ç¢ | å‡Œè¯ | ç ”è¯ | æ—¥æœˆå…‰ | è¯æ“ | è¯ç¢© |
    | --- | --- | --- | --- | --- | --- | --- |
    | åœ“å‰› | è‰¾è¨Š | å‹¤èª  | ä»å¯¶ | ä¸­å…‰é›» | å°é”é›» | ç›Šç™»ç§‘æŠ€ |
    | æ…§å‹ | é´»æµ· | å·¨å¤§ | æŠ€å˜‰ | è‹±æ¥­é” | è¿å»£ | å»£é‹ |
    | ç«‹ç«¯ | éº—è‡º | å…‰å¯¶ç§‘ | è¯ç™¼ç§‘ | ç¥é” | å¾®æ˜Ÿ | æ–°æ¼¢ |
    | å’Œç¢© | å»£é” | æ‰€ç¾…é–€ | é£›æ· | å¾·å¾‹ | æ›œè¶Š | é†«æš |
    | å°ç©é›» | è¯é›» | æ¬£èˆˆ | ç·¯å‰µ | ç·¯ç© | è°æ³° |  |
2. å–å¾—ç›¸é—œæ–°èè³‡è¨Š
    
    ä½¿ç”¨ `newsapi` å–å¾—æœ‰é—œå…¬å¸çš„æ–°èè³‡è¨Šï¼Œä¸¦æ•´ç†æˆ `.json` æª”ï¼Œæ¯ä¸€ç­†è³‡æ–™æ•´ç†çš„æ ¼å¼å¦‚ä¸‹ï¼Œä½†ä¸¦ä¸æ˜¯æ¯å€‹å…¬å¸éƒ½æœ‰ä¸­æ–‡å’Œè‹±æ–‡çš„æ–°èè³‡è¨Š
    
    ```json
    {
    	"name_zh": "å®ç¢",
    	"name_en": "Acer",
    	"news_zh": [...], 
    	"news_en": [...],
    	"code": "2353"
    }
    ```
    
3. ä½¿ç”¨ `twstock` å–å¾—ç›®æ¨™å…¬å¸ 2024/12 è‡³ä»Šçš„æ‰€æœ‰è‚¡ç¥¨åƒ¹æ ¼è³‡æ–™ï¼Œä¸¦è¨˜éŒ„åˆ° `.csv` ä¸­
    
    ```python
    ,date,capacity,turnover,open,high,low,close,change,transaction
    0,2025-01-02,53594040,1940182343,37.45,37.55,35.5,35.5,-2.15,28766
    1,2025-01-03,13816873,500942875,36.25,36.5,36.05,36.2,0.7,5900
    2,2025-01-06,11107457,409334732,36.55,37.1,36.3,36.95,0.75,6129
    ...
    ```
    
4. Prompt Template
    - è‡ªè¡Œè¨­è¨ˆ: æ¯æ®µå°è©±éƒ½æœ‰ `content` å’Œ `role` ï¼Œ `content` è¡¨ç¤ºå°è©±å…§å®¹ï¼Œ `role` æ˜¯å°è©±è…³è‰²ï¼Œåˆ†åˆ¥ç‚º`user` å’Œ `assistant` ï¼Œ `user` æ˜¯æŒ‡è¼¸å…¥è³‡æ–™çš„é‚£ä¸€æ–¹ï¼Œ `assistant` çš„å°è©±å…§å®¹å‰‡æ˜¯ LLM é æœŸè¼¸å‡ºæœï¼Œå°±åƒæ˜¯ä½¿ç”¨è€…å’Œ LLM å°è©±ï¼Œé€éä¸æ–·å‘Šè¨´ LLM è¼¸å…¥å•é¡Œå’Œé æœŸè¼¸å‡ºä¾†é€²è¡Œ fine-tuning
        - éå»å¹¾å¤©å·²çŸ¥è‚¡ç¥¨åƒ¹æ ¼åŒ…å«: æ—¥æœŸã€é–‹ç›¤åƒ¹ã€æœ€é«˜åƒ¹ã€æœ€ä½åƒ¹ã€æ”¶ç›¤åƒ¹ã€æ•¸é‡ç­‰
        - æç¤º LLM é æ¸¬å¾Œå¹¾å¤©çš„åƒ¹æ ¼
        
        ```json
            [
                {
                    "content": "Here is past 1 day stock price of å·¨å¤§ company.\n\nDate: 2025-01-03, Open: 141.5, High: 143.5, Low: 138.0, Close: 139.0, Capacity: 1447003\nPredict the next date price:\n",
                    "role": "user"
                },
                {
                    "content": "The next 1 day price is: Date:\n 2025-01-06, Open: 139.5, High: 144.5, Low: 137.5, Close: 143.0, Capacity: 1208884\n",
                    "role": "assistant"
                }
            ],
        ```
        
    - ç”¨ä»¥ Llama ä¾†æºçš„ tokenizer å‡½æ•¸ `apply_chat_template()` ï¼Œå¯ä»¥ç¢ºä¿è¼¸å…¥æ ¼å¼å’Œæ¨¡å‹è¨“ç·´æ ¼å¼æ˜¯ä¸€æ¨£çš„ï¼Œä¾‹å¦‚æœƒåŠ ä¸Š special token åœ¨å­—ä¸²ä¸­å¦‚ä¸‹
        
        ```python
        '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nHere is past 1 day stock price of å·¨å¤§ company.\n\nDate: 2025-01-09, Open: 145.0, High: 145.0, Low: 138.0, Close: 138.0, Capacity: 1697605\nPredict the next date price:\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nThe next date price is: Date:\n 2025-01-10, Open: 138.5, High: 144.5, Low: 137.5, Close: 143.5, Capacity: 1740644\n<|eot_id|>'
        ```
        
5. é æ¸¬æ–¹å¼
    
    å› ç‚ºæˆ‘è¦ç”¨å‰å¹¾å¤©çš„åƒ¹æ ¼å»é æ¸¬å¾Œå¹¾å¤©çš„åƒ¹æ ¼ï¼Œæœ€é•·æ˜¯çµ¦ä¸‰å€‹æœˆä¸¦é æ¸¬æ•´å€‹æœˆçš„è‚¡ç¥¨è³‡è¨Šï¼Œä»¥ä¸‹æœƒå°‡æ‰€æœ‰è³‡æ–™æ··å’Œæ•´ç†åˆ° `.json` æª”ä¸­ï¼Œç„¶å¾Œä¸€èµ·è¼¸å…¥åˆ° LLM ä¸­é€²è¡Œå¾®èª¿
    
    - æ ¹æ“šå‰ 1 å¤©é æ¸¬ä¸‹ 1 å¤©åƒ¹æ ¼
    - æ ¹æ“šå‰ 5 å¤©é æ¸¬å¾Œ 5 å¤©æ‰€æœ‰åƒ¹æ ¼
    - æ ¹æ“šå‰ä¸€å€‹æœˆé æ¸¬å¾Œ 5 å¤©æ‰€æœ‰åƒ¹æ ¼
    - æ ¹æ“šå‰ä¸‰å€‹æœˆé æ¸¬å¾Œ 1 å€‹æœˆæ‰€æœ‰åƒ¹æ ¼

## Fine-tuning

![image.png](static/1.png)

### Model

| Model | æè¿° |
| --- | --- |
| [**Llama-3.2-3B-Instruct**](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) | Meta æ¨å‡ºé–‹æºæ¨¡å‹çš„å…¶ä¸­ä¸€å€‹ï¼Œå…·æœ‰ 3.21 å„„å€‹åƒæ•¸ï¼Œæ”¯æŒå¤šèªè¨€å¾®èª¿ï¼Œä¸”å»¶çºŒ Llama-2 æ˜¯ç”¨ äººé¡å›é¥‹çš„å¼·åŒ–å­¸ç¿’ (RLHF) é è¨“ç·´ï¼Œåœ¨æœ‰é™è³‡æºæ¢ä»¶ä¸‹ï¼Œæ‰€ä»¥é¸æ“‡æ­¤æ¨¡å‹é€²è¡Œ ç›£ç£å¾®èª¿ (SFT) ä¾†é æ¸¬ä¸åŒå€é–“çš„è‚¡ç¥¨è³‡è¨Š |
| [Llama-3-8b-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-bnb-4bit) | é€™å€‹æ˜¯ `unsloth` åŸºæ–¼ Meta é–‹ç™¼çš„ Llama3 æ¨¡å‹ç”¨ [4-bit é‡åŒ–æŠ€è¡“](https://huggingface.co/docs/bitsandbytes/reference/nn/linear4bit) ï¼Œ å°±æ˜¯æŠŠæµ®é»æ•¸è½‰ç‚ºåªç”¨ 4 å€‹ä½å…ƒå°±å¯ä»¥å­˜æ”¾çš„æ•¸å€¼ï¼Œé™ä½å„²å­˜ç©ºé–“å’Œè¨“ç·´æ‰€éœ€ VRAMï¼Œå¯ä»¥ç”¨è¼ƒå°‘çš„è³‡æºå»å¾®èª¿å¤§èªè¨€æ¨¡å‹ |

### æ¸›å°‘é‹ç®—è³‡æºçš„ä½¿ç”¨

1. ä½¿ç”¨  [Unsloth](https://docs.unsloth.ai/) å¥—ä»¶ä¾†å¾®èª¿é–‹æºæ¨¡å‹ [llama-3.2-3B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)  
    1. å®‰è£å¥—ä»¶
        
        ```
        %%capture
        !pip install unsloth
        # Also get the latest nightly Unsloth!
        !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git
        ```
        
    2. å¯é¸æ“‡çš„å¾®èª¿æ¨¡å‹: å¯åˆ°å®˜ç¶²æŸ¥çœ‹æ›´å¤šæ”¯æŒçš„æ¨¡å‹ï¼Œæ­¤æ¬¡åªä½¿ç”¨  `unsloth/Meta-Llama-3.1-8B-bnb-4bit` å’Œ  `unsloth/Llama-3.2-3B-Instruct`  
        
        ```python
        "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 2x faster
        "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
        "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
        "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 4bit for 405b!
        "unsloth/Mistral-Small-Instruct-2409",     # Mistral 22b 2x faster!
        "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
        "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
        "unsloth/Phi-3-medium-4k-instruct",
        "unsloth/gemma-2-9b-bnb-4bit",
        "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
        "unsloth/Llama-3.2-1B-bnb-4bit",           # NEW! Llama 3.2 models
        "unsloth/Llama-3.2-1B-Instruct-bnb-4bit",
        "unsloth/Llama-3.2-3B-bnb-4bit",
        "unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
        "unsloth/Llama-3.3-70B-Instruct-bnb-4bit" # NEW! Llama 3.3 70B!
        ```
        
    3. Llama ç³»åˆ—é–‹æºæ¨¡å‹: è‹¥æ˜¯è¦å¾ hugging face ä¸Šå–å¾—æ¨¡å‹ï¼Œå› ç‚º Meta è¦é˜²æ­¢ä»–å€‘çš„é–‹æºæ¨¡å‹å•†ç”¨ä»¥åŠå¾ä¸­ä¸æ³•ç²åˆ©ï¼Œéœ€ç”³è«‹æ¨¡å‹ä½¿ç”¨çš„ token æ‰å¯ä»¥ä½¿ç”¨æ¨¡å‹
        1. å¡«å¯«ç”³è«‹è¡¨
            
            ![image.png](static/2.png)
            
        2. æœ€å¾Œé¡¯ç¤ºå·²å–å¾—é€™å€‹æ¨¡å‹ï¼Œå°±å¯ä»¥ä½¿ç”¨å‰›å‰›çš„ token åœ¨æœ¬åœ°ç«¯ç”¨ hugging face å–å¾—é€™å€‹æ¨¡å‹
            
            ![image.png](static/3.png)
            
    4. è¼‰å…¥é è¨“ç·´æ¨¡å‹
        
        å¯ä»¥è¨­å®šåƒæ•¸ `load_in_4bit` æˆ–æ˜¯ `load_in_8bit` ä¾†é¸æ“‡é‡åŒ–çš„æ–¹æ¡ˆï¼Œå¦‚æœå°‡ `load_in_4bit` è¨­ç½®ç‚º `True` ï¼Œå°±è¡¨ç¤ºæœƒå°‡é€™å€‹æ¨¡å‹æ¬Šé‡é‡åŒ–æˆ 4 å€‹ä½å…ƒå°±èƒ½å„²å­˜çš„æ•¸å€¼ï¼Œé›–ç„¶é€™æ¨£å°‡åŸæœ¬çš„æµ®é»æ•¸è½‰ç‚ºç²¾åº¦è¼ƒå°çš„å€¼ï¼Œä½†é€™æ¨£å»å¯ä»¥é™ä½å¾ˆå¤šé‹ç®—æˆæœ¬ï¼Œè®“ä¸€èˆ¬æ²’æœ‰å¤ªå¤šé‹ç®—è³‡æºçš„ä½¿ç”¨è€…å¯ä»¥é€²å…¥å¾®èª¿ LLM çš„è¨“ç·´ä¸­ï¼Œ `load_in_8bit` äº¦æ˜¯
        
        ```python
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
            max_seq_length = max_seq_length,
            dtype = dtype,
            load_in_4bit = True,
        )
        ```
        

### Parameter-Efficient Fine-Tuning (PEFT)

PEFT æ–¹æ³•å¯ä»¥é€éå¾®èª¿å°‘é‡æ¨¡å‹åƒæ•¸è€Œä¸æ˜¯æ‰€æœ‰æ¨¡å‹åƒæ•¸ï¼Œä½¿å¤§å‹é è¨“ç·´å¤§èªè¨€æ¨¡å‹èƒ½å¤ æœ‰æ•ˆåœ°é©æ‡‰å„ç¨®ä¸‹æ¸¸æ‡‰ç”¨ï¼Œè€Œä¸”å¯ä»¥å¤§å¤§é™ä½è¨ˆç®—æˆæœ¬ï¼ŒPEFT æ˜¯ä¸€ç¨®çµ±ç¨±ï¼Œä»–åº•ä¸‹æœ‰ä¸€å †å¾®èª¿æŠ€è¡“ï¼Œä»¥ä¸‹æ˜¯ 2019 è‡³ 2023 å¹´çš„æŠ€è¡“æ¼”é€²ï¼Œå…¶ä¸­ LoRA æ˜¯ä¸€å€‹å»£ç‚ºä½¿ç”¨çš„ PEFT æ–¹æ³•ï¼Œç›®å‰ hugging face å·²ç¶“æœ‰å¯ä»¥ç›´æ¥ä½¿ç”¨ [PEFT çš„å¥—ä»¶](https://huggingface.co/docs/peft/index)ï¼Œå¯ä»¥é€éé€™äº›å¥—ä»¶é¸æ“‡ä½¿ç”¨ä»»æ„æŠ€è¡“ä¾†å¾®èª¿ LLMï¼Œè€Œæ­¤æ¬¡å°‡ä½¿ç”¨ä¸Šè¿°çš„ `unsloth` å¥—ä»¶èˆ‡ LoRA æŠ€è¡“ä¾†å¾®èª¿ Llama æ¨¡å‹

![Xu, L., Xie, H., Qin, S.J., Tao, X., & Wang, F.L. (2023). Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment.Â *ArXiv, abs/2312.12148*.](static/4.png)

Xu, L., Xie, H., Qin, S.J., Tao, X., & Wang, F.L. (2023). Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment.Â *ArXiv, abs/2312.12148*.

### Low Rank Adaptation (LoRA)

1. åƒè€ƒç”±å¾®è»Ÿæ‰€ç™¼è¡¨çš„è«–æ–‡: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) ä¸­çš„çš„æ–¹æ³•ï¼ŒLoRA æ˜¯ PEFT  ä¸­çš„ä¸€ç¨®æ–¹æ³•ï¼Œå¯ä»¥ä¸ç”¨è¨“ç·´å…¨éƒ¨åƒæ•¸ï¼Œåªè¦è¨“ç·´æ©˜è‰²éƒ¨åˆ† $A$ å’Œ $B$ çš„éƒ¨åˆ†å°±è¡Œï¼Œè—è‰²çš„ $W$ å°±æ˜¯ ä¸ç”¨è¨“ç·´
    
    ![Hu, J.E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., & 
    Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. *ArXiv, abs/2106.09685*.](static/5.png)
    
    Hu, J.E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., & 
    Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. *ArXiv, abs/2106.09685*.
    
2. æ•¸å­¸è§£é‡‹
    
    æ¯ä¸€å±¤çš„è¼¸å‡º $h$ æœƒç­‰æ–¼ä¸€é–‹å§‹è¼¸å…¥çš„ $W_0$ (ä¹Ÿå°±æ˜¯é è¨“ç·´æ¬Šé‡) åŠ ä¸Šé€™ä¸€å±¤ç®—å®Œçš„ $\Delta W$ 
    
    $$
    â
    $$
    
    æ­¤æ™‚å›ºå®šé è¨“ç·´æ¬Šé‡ $W_0$ çš„éƒ¨åˆ†ï¼Œåªç®— $\Delta W$ ï¼Œä¸¦ç”¨ ç¸®æ”¾å› å­ $\frac{\alpha}{r}$ æ§åˆ¶æ¬Šç¨®è¼¸å‡ºï¼Œå¯ä»¥æœ‰é¡ä¼¼å­¸ç¿’ç‡çš„æ•ˆæœ
    
    - $\frac{\alpha}{r} = 1$ : å¯¦éš›ç¾¤çœ¾æ˜¯å¤šå°‘å°±æ˜¯å¤šå°‘
    - $\frac{\alpha}{r}>1$ : æ”¾å¤§æ¬Šé‡ï¼ŒåŠ å¼· LoRA è¨“ç·´æ•ˆæœ
    - $\frac{\alpha}{r}<1$ : ç¸®å°æ¬Šé‡ï¼Œæ¸›å¼± LoRA è¨“ç·´æ•ˆæœï¼Œå¯èƒ½æ˜¯é˜²æ­¢ overfitting
    
    $$
    W_{final} = W_0 + (\frac{\alpha}{r}) \cdot \Delta W
    $$
    
    æ‰€ä»¥æˆ‘å€‘åªæœƒè¨“ç·´ $\Delta W$ çš„éƒ¨åˆ†ï¼Œä¸¦ä¸æœƒæ•´å€‹ $W_{final}$ éƒ½è¨“ç·´ï¼Œä¸¦ä¸”ä½¿ç”¨å…©å€‹ low-rank çš„çŸ©é™£$B \in R^{d\times r}$  å’Œ $A \in R^{r\times k}$   ä¾†é‹ç®—  $\Delta W$ ï¼Œé€™è£¡çš„ $r$ å°±æ˜¯æŒ‡çŸ©é™£çš„ rank
    
    $$
    \Delta W = BA
    $$
    
3. å¯¦ä½œ LoRA model
    
    ä»¥ä¸‹æ˜¯é€é  `unsloth`  ä¸­çš„ç‰©ä»¶  `FastLanguageModel`  ä¾†å¯¦ä½œ LoRA æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥è¨­å®šå°æ‡‰ä¸Šåœ–çš„åƒæ•¸ï¼Œå…¶ä¸­
    
    - `r` :  å°±æ˜¯çŸ©é™£çš„ rankï¼Œå¯ä»¥çœ‹æˆçŸ©é™£çš„å¤§å°
    - `target_module` : è¦é€²è¡Œ LoRA å¾®èª¿çš„æ¨¡çµ„ï¼Œé€™é‚Šå°±é¸æ“‡è¨ˆç®— Attention çš„ä¸‰å€‹å…ƒç´  query `q` ã€ Key `k` ã€ Value `v` ï¼Œå› ç‚ºè¨ˆç®— Attention éç¨‹ $QK$ å…©å€‹çŸ©é™£ç›¸ä¹˜çš„è¨ˆç®—é‡æœƒå¾ˆå¤§ï¼Œé‚„æœ‰ Feedforward å±¤ `up/down` ã€ `gate`
    - `lora_alpha` : å¯ä»¥åƒè€ƒè©²è«–æ–‡ï¼Œå…¶ä¸­æåŠç¸®æ”¾ $\Delta W$ æ™‚æ‰€ç”¨åˆ°çš„æ•¸å€¼  $\frac{\alpha}{\gamma}$  ä¸­çš„ $\alpha$
    
    ```python
    model = FastLanguageModel.get_peft_model(
        model,
        r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj",],
        lora_alpha = 16,
        lora_dropout = 0, # Supports any, but = 0 is optimized
        bias = "none",    # Supports any, but = "none" is optimized
        # "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
        use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
        random_state = 3407,
        use_rslora = False,  # Unsloth support rank stabilized LoRA
        loftq_config = None, # And LoftQ
    )
    ```
    

### Supervised fine-tuning `SFTTrainer`

æˆ‘çš„ç­–ç•¥æ˜¯åˆ©ç”¨ç„¡ç›£ç£å¼å­¸ç¿’é è¨“ç·´çš„æ¨¡å‹ï¼Œé€²è¡Œä¸‹æ¸¸çš„ç›£ç£å¼å¾®èª¿ï¼Œæ‰€ä»¥ä½¿ç”¨å¥—ä»¶ `trl` ä¸­çš„ [`SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer) ä¾†æ›´æ•ˆç‡é€²è¡Œå¾®èª¿ï¼Œç”¨å‰›å‰›æº–å‚™çš„è³‡æ–™ä»¥åŠæ¨¡å‹ç‚ºåƒæ•¸å»è¨“ç·´

```python
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = hf_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),
    dataset_num_proc = 2,
    packing = False, 
    args = TrainingArguments(
        per_device_train_batch_size = 4,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "tensorboard", 
    ),
)
```

æ›´å¤šçš„åƒæ•¸è¨­å®šå¯ä»¥åƒè€ƒ [hugging face çš„æ–‡ä»¶](https://huggingface.co/docs/trl/sft_trainer#trl.ModelConfig) ï¼Œå¦‚æœæœ‰é‡åˆ° `TypeError: SFTTrainer.__init__() got an unexpected keyword argument 'dataset_text_field'Â #1264` çš„å•é¡Œå¯ä»¥åƒè€ƒ GitHub é€™å€‹ [è¨è«–](https://github.com/unslothai/unsloth/issues/1264)

### è¨“ç·´åƒæ•¸

| åƒæ•¸ | æ•¸å€¼ |
| --- | --- |
| Num examples | 17325 |
| Epoch number | 1 |
| Step | 60 |
| *Batch size per device* | 8 |
| *Gradient accumulation steps* | 4 |
| Num GPUs used | 1 |
| Total batch size  | (8 x 4 x 1) = 32 |
| Trainable parameters | 41,943,040/8,000,000,000 (0.52% trained) |

## Model Performance and Analysis

### Training Results

1. è¼¸å…¥ä¸åŒæ¬„ä½çš„è‚¡ç¥¨è³‡è¨Š
    1. é¡è‰²
        1. ğŸŸ§ æ©˜è‰²: `['date', 'capacity', 'turnover', 'open', 'high', 'low', 'close', 'change', 'transaction']` 
        2. ğŸŸ¦ æ·±è—è‰²: `['date', 'turnover', 'open', 'high', 'low', 'close', 'transaction']`
        3. ğŸŸ¥ ç´…è‰²: `['date', 'open', 'high', 'low', 'close', 'transaction']` 
    2. æå¤±å‡½æ•¸æ²’æœ‰æ˜é¡¯å·®ç•°
        
        ![image.png](static/6.png)
        
    3. æ¢¯åº¦èŒƒæ•¸: å‘ˆç¾æ¢¯åº¦å¤§å°ï¼Œä¹Ÿæœ‰æ­£å¸¸æ”¶æ–‚
        
        ![image.png](static/7.png)
        
2. æ˜¯å¦æœ‰ Context
    
    ğŸŸ¦ æ·ºè—è‰²æŠ˜ç·šç‚ºæ²’æœ‰ context çš„æå¤±å‡½æ•¸ï¼Œå°æ¯”å…¶ä»–æœ‰ context çš„ loss è¡¨ç¾æ²’æœ‰æ˜é¡¯å·®ç•°
    
    ![image.png](static/8.png)
    
3. ä¸åŒå­¸ç¿’ç‡èª¿æ•´ç­–ç•¥
    1. çµ±ä¸€ç”¨æ¬„ä½ `['date', 'turnover', 'open', 'high', 'low', 'close', 'transaction']`
    2. é¡è‰²
        1. ğŸŸ¦ è—è‰²   : ä½¿ç”¨ cosine
        2. ğŸŸ¥ ç²‰ç´…è‰²: ä½¿ç”¨ linear
    3. loss function: æ²’å•¥å·®åˆ¥
        
        ![image.png](static/9.png)
        
    4. Gradient norm: **cosine** çš„ gradient norm ç¨ä½ï¼Œä»£è¡¨è¨“ç·´éç¨‹å¯èƒ½æ›´ç©©å®šæˆ–æ›´å®¹æ˜“æ”¶æ–‚ã€‚
        
        ![image.png](static/10.png)
        
    
4. ä¸åŒ LoRA åƒæ•¸
    
    

### Inference Results


## Code Availability

### File

| Code | Detail |
| --- | --- |
| `main.py` | Main file to execute training and inference process. |
| `model.py`  | LSTM and Llama model are defined here |
| `stock_api.py` | function api to   http://140.116.86.242:8081/stock/api/v1    including buy, sell, and get information of user |
| `utils.py` | utility functions are defined here |
| `main.py`  | main control of deciding operation to stock. |
| `dataset.py` | Define class object for stock prices data and newspaper data. |
| `stock_predictor.py` | Stock decision process: data preprocessing â†’ training â†’ predict future prices |

### Arguments

1. Mode
    
    
    | Name | Detail |
    | --- | --- |
    | `--train`  | 1 â†’ è¨“ç·´æ¨¡å¼, 0 â†’ æ¨è«–æ¨¡å¼ |
2. Data
    
    
    | Name | Detail |
    | --- | --- |
    | `--data-dir` | è³‡æ–™å­˜æ”¾çš„è·¯å¾‘ |
    | `--company-list-file` | è¦è¨“ç·´è‚¡ç¥¨è³‡è¨Šçš„ç›®æ¨™å…¬å¸çš„ `.json` æª”æ¡ˆ |
    | `--year` | è‡ªå‹•æ”¶é›†ç›®æ¨™å…¬å¸çš„è‚¡ç¥¨éœ€è¦çš„æ™‚é–“: å¹´ |
    | `--month` | è‡ªå‹•æ”¶é›†ç›®æ¨™å…¬å¸çš„è‚¡ç¥¨éœ€è¦çš„æ™‚é–“: æœˆ |
    | `--choose-stock` | å¦‚æœæ²’æœ‰è‡ªè¡Œæº–å‚™ç›®æ¨™å…¬å¸æª”æ¡ˆï¼Œç³»çµ±å¯ä»¥éš¨æ©Ÿé¸å– `choose-stock` æª”ä¸Šå¸‚è‚¡ç¥¨ |
3. Load pretrained model
    
    
    | Name | Detail |
    | --- | --- |
    | `--model-name` | ç›®å‰æœ‰ `unsloth/Llama-3.2-3B-Instruct`  å’Œ `unsloth/Meta-Llama-3.1-8B-bnb-4bit`  å…©ç¨®æ¨¡å‹å¯ä»¥ä½¿ç”¨ |
    | `--max-seq-length` | é™åˆ¶ç”Ÿæˆåºåˆ—çš„æœ€é•·é•·åº¦ |
    | `--load_in_4bit` | æ¨¡å‹ä»¥ 4-bit è¼‰å…¥ |
    | `--load_in_8bit` | æ¨¡å‹ä»¥ 8-bit è¼‰å…¥ |
    | `--model-dir` | è¨“ç·´å®Œæ¨¡å‹å„²å­˜çš„è·¯å¾‘ |
4. LoRA
    
    
    | Name | Detail |
    | --- | --- |
    | `--rank` | ä»¥ LoRA è¨“ç·´ä¸­çš„ $r$ ï¼Œæ±ºå®šäº† $A$ å’Œ $B$ çš„å¤§å° |
    | `--lora-alpha` | LoRA è¨“ç·´ä¸­çš„ $\alpha$  |
    | `--lora-dropout` | LoRA çš„ dropout rate |
    | `--random-state` | è¨­ç½®éš¨æ©Ÿç¨®å­ï¼Œæ¯æ¬¡åŸ·è¡Œè¨“ç·´æˆ–å¾®èª¿æ™‚ï¼Œæ¨¡å‹çš„è¡Œç‚ºèˆ‡çµæœä¸€è‡´ |
5. Trainer
    
    
    | Name | Detail |
    | --- | --- |
    | `--batch-size` | æ¯æ¬¡æ¨¡å‹è¨“ç·´æ™‚ï¼Œä¸€æ¬¡è™•ç†å¤šå°‘ç­†è³‡æ–™ |
    | `--gradient-accumulation-steps` | ç´¯ç©å¹¾å€‹ batch çš„æ¢¯åº¦å†æ›´æ–°ä¸€æ¬¡æ¨¡å‹æ¬Šé‡ |
    | `--num-epochs` | è¨“ç·´æ¨¡å‹å¹¾æ¬¡ï¼Œä¸€å€‹ epoch å°±æ˜¯æ¨¡å‹çœ‹éä¸€éæ‰€æœ‰è¨“ç·´è³‡æ–™çš„éç¨‹ |
    | `--max-steps` | è¨­å®šç¸½å…±è¦é€²è¡Œå¤šå°‘æ­¥ï¼ˆstepsï¼‰è¨“ç·´ |
    | `--learning-rate` | æ¯æ¬¡æ¨¡å‹æ›´æ–°æ™‚ï¼Œæ¬Šé‡è¦èª¿æ•´å¤šå°‘ï¼Œå°±æ˜¯å­¸ç¿’æ­¥ä¼çš„å¤§å° |
    | `--logging-steps` | æ¯éš”å¤šå°‘å€‹ training stepï¼Œè¦è¼¸å‡ºä¸€æ¬¡è¨“ç·´æ—¥èªŒ |
    | `--optim` | ä½¿ç”¨å“ªä¸€ç¨® å„ªåŒ–å™¨ï¼ˆoptimizerï¼‰ ä¾†è¨“ç·´æ¨¡å‹ |
    | `--weight-decay` | æ¬Šé‡è¡°æ¸›æ˜¯ä¸€å€‹æ­£å‰‡åŒ–æŠ€è¡“ï¼Œç”¨ä¾†é˜²æ­¢æ¨¡å‹ overfitting |
    | `--lr-scheduler-type` | å­¸ç¿’ç‡å¦‚ä½•éš¨è¨“ç·´é€²è¡Œè€Œèª¿æ•´çš„ç­–ç•¥ï¼Œæ±ºå®šå­¸ç¿’ç‡çš„è®ŠåŒ–æ›²ç·š |
    | `--output-dir` | æ¨¡å‹è¨“ç·´è¼¸å‡ºçµæœè¦å„²å­˜åœ¨å“ªå€‹è³‡æ–™å¤¾ï¼Œå¯ä»¥ç”¨æ–¼ `tensorboard` çš„å‘ˆç¾ |
    | `--report-to` | è¨“ç·´éç¨‹ä¸­è¦å°‡è¨“ç·´æŒ‡æ¨™ï¼ˆå¦‚ lossã€learning rate ç­‰ï¼‰å›å ±åˆ°å“ªè£¡ï¼Œå¦‚æœ¬æ¬¡å¯¦é©—æ˜¯ç”¨ `tensorboard` |

### Get Started
å®‰è£æ‰€éœ€å¥—ä»¶
```bash
pip install -r requirements.txt 
```
1. è‡ªè¡Œæº–å‚™è³‡æ–™
    1.  å°‡æƒ³è¦è¨“ç·´çš„å…¬å¸åå­—æˆ–æ˜¯è‚¡ç¥¨ç·¨è™Ÿæ”¾åˆ° `.json` æª”æ¡ˆä¸­
        
        ```json
        [
            {"name_zh": "å°ç©é›»"},
            {"name_zh": "è¯ç™¼ç§‘"},
            {"code": 3711}, 
            ...
        ]
        ```
        
    2. è¨“ç·´
        
        ```bash
        python main.py --train 1 --company-list-file <json æª”å>
        ```
        
2. ç³»çµ±éš¨æ©ŸæŒ‘é¸è³‡æ–™
    
    è¨­ç½®åƒæ•¸ `--choose-stock` ï¼Œç³»çµ±æœƒéš¨æ©ŸæŒ‘é¸ä¸¦æ”¶é›†å››å¤§è²·é»å’Œå››å¤§è³£é»çš„å„  `num`  æª”ä¸Šå¸‚è‚¡ç¥¨è³‡æ–™å»é€²è¡Œè¨“ç·´
    
    ```bash
    python main.py --train 1 --choose-stock <num>
    ```
    
3. æŸ¥çœ‹è¨“ç·´è¡¨ç¾
    - å®‰è£ `tensorboard`
        
        ```bash
        pip install tensorboard
        ```
        
    - Launch TensorBoard
        
        ```bash
        tensorboard --logdir=outputs/runs
        ```
        
    - æ‰“é–‹ http://localhost:6006/  å³å¯å³æ™‚è§€å¯Ÿè¨“ç·´éç¨‹
        
        ![image.png](static/11.png)
        

### Inference

1. ä½¿ç”¨å·²ç¶“å¾®èª¿å¥½çš„æ¨¡å‹é€²è¡Œé æ¸¬è‚¡ç¥¨
    - å…¬å¸çš„è‚¡ç¥¨ç·¨è™Ÿ `code` : å¯ä»¥åˆ° [å°ç£è­‰åˆ¸äº¤æ˜“æ‰€å®˜ç¶²çš„æŸ¥è©¢ç³»çµ±](https://isin.twse.com.tw/isin/single_i.jsp)  æŸ¥è©¢è‚¡ç¥¨ç·¨è™Ÿï¼Œä»¥å­—ä¸²æ¨¡å¼è¼¸å…¥
    - è¼¸å…¥éå»å¤©æ•¸ï¼Œå’Œæƒ³è¦é æ¸¬è‚¡ç¥¨çš„æœªä¾†å¤©æ•¸
    
    ```bash
    StockPredictor.inference(self, code='2330', past=5, future=5, context="")
    ```
    
2. éå»èˆ‡æœªä¾†è¶¨å‹¢æ›²ç·šåœ–
    
    æœƒå°‡è¼¸å…¥éå»å¤©æ•¸å’Œé æ¸¬çš„æœªä¾†å¤©æ•¸çš„è‚¡ç¥¨è³‡è¨Šç•«æˆ k ç·šåœ–ï¼Œ
    

## Future Work

1. èˆ‡å…¶ä»–å‚³çµ±æ·±åº¦å­¸ç¿’ CNN, RNN ç­‰æ¨¡å‹åšæ¯”è¼ƒ
2. å¯ä»¥é€éæ‡‰ç”¨ä»‹é¢æ›´å®¹æ˜“æ“ä½œ

## Reference

- [**What's 4-bit quantization? How does it help Llama2**](https://www.kaggle.com/code/lorentzyeung/what-s-4-bit-quantization-how-does-it-help-llama2)
- [**Rank-Stabilized LoRA: Unlocking the Potential of LoRA Fine-Tuning**](https://huggingface.co/blog/damjan-k/rslora)
- [Llama-3.2 1B+3B Conversational + 2x faster finetuning](https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing)
- [è‡ºç£è­‰åˆ¸äº¤æ˜“æ‰€ OpenAPI](https://openapi.twse.com.tw/)